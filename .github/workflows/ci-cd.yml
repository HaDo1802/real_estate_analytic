name: Real Estate ETL Pipeline CI/CD

on:
  schedule:
    - cron: "0 6 * * *" # Daily at 6 AM UTC
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      run_full_pipeline:
        description: "Run full ETL pipeline (not just tests)"
        required: false
        default: "false"
        type: choice
        options:
          - "true"
          - "false"

env:
  PYTHON_VERSION: "3.9"

jobs:
  format-and-lint:
    name: Code Format & Lint
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install formatting tools
        run: |
          python -m pip install --upgrade pip
          pip install black isort

      - name: Format code with Black
        run: |
          echo "ğŸ¨ Formatting code with Black..."
          black etl/ tests/ --line-length 127

      - name: Sort imports with isort
        run: |
          echo "ğŸ“¦ Sorting imports..."
          isort etl/ tests/ --profile black

      # No flake8, no linting failures, no noise
      # No auto-commit (pre-commit handles formatting locally)

  unit-test:
    name: Run Unit Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install pandas pytest
      
      - name: Run unit tests
        run: |
          pytest tests/test_*.py -v
      
      - name: Tests passed!
        run: echo "âœ… All tests passed!"
  
  docker-build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [format-and-lint, unit-test]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Log in to Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
      
      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: ${{ github.event_name != 'pull_request' && secrets.DOCKER_USERNAME != '' }}
          tags: ${{ secrets.DOCKER_USERNAME }}/real-estate-etl:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

  run-etl-pipeline:
    runs-on: ubuntu-latest
    name: Execute Real Estate ETL Pipeline
    needs: [format-and-lint, unit-test, docker-build]
    if: github.ref == 'refs/heads/main' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.run_full_pipeline == 'true'))
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_DB: test_real_estate
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_pass_123
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas requests psycopg2-binary python-dotenv
    
    - name: Create data directories
      run: |
        mkdir -p data/raw data/transformed etl_log
        chmod 777 data/raw data/transformed etl_log
    
    - name: Configure environment
      run: |
        # API Key (required for extract.py)
        echo "RAPID_API_KEY=${{ secrets.RAPID_API_KEY }}" > .env
        
        # Local database config (used by config.py when not in Docker)
        echo "POSTGRES_HOST_LOCAL=localhost" >> .env
        echo "POSTGRES_DB_LOCAL=test_real_estate" >> .env
        echo "POSTGRES_USER_LOCAL=test_user" >> .env
        echo "POSTGRES_PASSWORD_LOCAL=test_pass_123" >> .env
        echo "POSTGRES_PORT_LOCAL=5432" >> .env
        
        # Schema name
        echo "DEFAULT_SCHEMA=real_estate_data" >> .env
    
    - name: Step 1 - Extract Real Estate Data
      run: |
        echo "ğŸ•·ï¸ Starting Zillow data extraction..."
        cd etl
        python extract.py
        echo "âœ… Data extraction completed"
        
        if [ -f "../data/raw/raw_latest.csv" ]; then
          echo "ğŸ“„ Raw data file created successfully"
          echo "Raw data rows: $(wc -l < ../data/raw/raw_latest.csv)"
        else
          echo "âŒ Raw data file not found"
          exit 1
        fi
    
    - name: Step 2 - Transform and Clean Data
      run: |
        echo "ğŸ§¹ Starting data transformation..."
        cd etl
        python transform.py
        echo "âœ… Data transformation completed"
        
        if [ -f "../data/transformed/transformed_latest.csv" ]; then
          echo "ğŸ“„ Transformed data file created successfully"
          echo "Transformed data rows: $(wc -l < ../data/transformed/transformed_latest.csv)"
        else
          echo "âŒ Transformed data file not found"
          exit 1
        fi
    
    - name: Step 3 - Load to PostgreSQL
      run: |
        echo "ğŸ—„ï¸ Starting PostgreSQL load..."
        cd etl
        python load.py
        echo "âœ… PostgreSQL load completed"
    
    - name: Verify Data Load
      run: |
        # Verify data was loaded to PostgreSQL
        PGPASSWORD=test_pass_123 psql -h localhost -U test_user -d test_real_estate -c "\dt real_estate_data.*" || echo "âš ï¸ Could not list tables"
        PGPASSWORD=test_pass_123 psql -h localhost -U test_user -d test_real_estate -c "SELECT COUNT(*) as total_records FROM real_estate_data.properties_data_history;" || echo "âš ï¸ Could not count records"
    
    - name: Pipeline Summary
      run: |
        echo "ğŸ‰ Real Estate ETL Pipeline Completed Successfully!"
        echo "ğŸ“… Executed on: $(date)"
        echo "ğŸ“Š Pipeline Summary:"
        echo "  âœ… Data Extraction (Zillow API)"
        echo "  âœ… Data Transformation"
        echo "  âœ… PostgreSQL Load"
        
        echo ""
        echo "ğŸ“¦ Data Files:"
        ls -lh data/raw/raw_latest.csv 2>/dev/null || echo "  Raw data: Not found"
        ls -lh data/transformed/transformed_latest.csv 2>/dev/null || echo "  Transformed data: Not found"
    
    - name: Upload Data Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: real-estate-data-${{ github.run_number }}
        path: |
          data/raw/*.csv
          data/transformed/*.csv
          etl_log/*.txt
        retention-days: 30