name: Real Estate ETL Pipeline CI/CD

on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  push:
    branches: 
      - main      
    paths:
      # Only run if these paths change (saves GitHub Actions minutes)
      - 'etl/**'
      - 'tests/**'
      - 'dags/**'
      - 'requirements.txt'
      - '.github/workflows/**'
  

  pull_request:
    branches: 
      - main
  
 
  workflow_dispatch:
    inputs:
      run_full_pipeline:
        description: 'Run full ETL pipeline (not just tests)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'true'
          - 'false'
      max_properties:
        description: 'Maximum properties to extract (for testing)'
        required: false
        default: '100'


env:
  PYTHON_VERSION: '3.9'
  POSTGRES_VERSION: '13'
  CACHE_VERSION: 'v1'

jobs:

  code-quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
      
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
      
    - name: Install linting tools
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort pylint
      
    - name: Check code formatting with Black
      run: |
        echo "ğŸ¨ Checking code formatting..."
        black --check etl/ tests/ --line-length 127
      
    - name: Check import sorting with isort
      run: |
        echo "ğŸ“¦ Checking import sorting..."
        isort --check-only etl/ tests/ --profile black
      
    - name: Lint with flake8
      run: |
        echo "ğŸ” Running flake8 linting..."
        flake8 etl/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 etl/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest

    - name: Run unit tests
      run: |
        pytest tests/

  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        severity: 'CRITICAL,HIGH'
      
    - name: Upload Trivy results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'


  docker-build:
    name: Build & Validate Docker Image
    runs-on: ubuntu-latest
    needs: [unit-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build Docker image
      run: |
        echo "ğŸ³ Building Docker image..."
        docker build -t real-estate-etl:${{ github.sha }} .
      
    - name: Test Docker image
      run: |
        echo "ğŸ§ª Testing Docker image..."
        docker run --rm real-estate-etl:${{ github.sha }} python -c "
        import sys
        sys.path.insert(0, '/opt/airflow/etl')
        from transform import extract_address_components
        result = extract_address_components('123 Main St, Las Vegas, NV 89101')
        assert result['city'] == 'Las Vegas'
        print('âœ… Docker image validated')
        "

  run-etl-pipeline:
    name: Execute Real Estate ETL Pipeline
    runs-on: ubuntu-latest
    needs: [unit-tests, security-scan, docker-build]
    
    if: |
      github.ref == 'refs/heads/main' && 
      (github.event_name == 'schedule' || 
       (github.event_name == 'workflow_dispatch' && 
        github.event.inputs.run_full_pipeline == 'true'))
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create data directories
      run: |
        mkdir -p data/raw data/transformed etl_log
        chmod -R 777 data/ etl_log/
        
    - name: Configure environment
      run: |
        echo "RAPID_API_KEY=${{ secrets.RAPID_API_KEY }}" > .env
        echo "POSTGRES_HOST=${{ secrets.POSTGRES_HOST }}" >> .env
        echo "POSTGRES_DB=${{ secrets.POSTGRES_DB }}" >> .env
        echo "POSTGRES_USER=${{ secrets.POSTGRES_USER }}" >> .env
        echo "POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD }}" >> .env
        echo "POSTGRES_PORT=${{ secrets.POSTGRES_PORT }}" >> .env
        echo "SENDER_EMAIL=${{ secrets.SENDER_EMAIL }}" >> .env
        echo "SENDER_PASSWORD=${{ secrets.SENDER_PASSWORD }}" >> .env
      
    - name: Step 1 - Extract Real Estate Data
      run: |
        echo "ğŸ•·ï¸ Starting data extraction..."
        cd etl
        python extract.py
        
        if [ -f "../data/raw/raw_latest.csv" ]; then
          ROW_COUNT=$(wc -l < ../data/raw/raw_latest.csv)
          echo "ğŸ“Š Properties extracted: $((ROW_COUNT - 1))"
        else
          echo "âŒ Extraction failed"
          exit 1
        fi
      
    - name: Step 2 - Transform & Clean Data
      run: |
        echo "ğŸ§¹ Starting data transformation..."
        cd etl
        python transform.py
        
        if [ -f "../data/transformed/transformed_latest.csv" ]; then
          CLEAN_COUNT=$(wc -l < ../data/transformed/transformed_latest.csv)
          echo "ğŸ“Š Records processed: $((CLEAN_COUNT - 1))"
        else
          echo "âŒ Transformation failed"
          exit 1
        fi
      
    - name: Step 3 - Load to PostgreSQL
      run: |
        echo "ğŸ—„ï¸ Loading data to PostgreSQL..."
        cd etl
        python load.py
        echo "âœ… Database load completed"
      
    - name: Pipeline Summary
      run: |
        echo "ğŸ‰ ETL Pipeline Completed Successfully!"
        echo "ğŸ“… Executed on: $(date)"
        echo "ğŸ“Š Pipeline Summary:"
        echo "  âœ… Data Extraction"
        echo "  âœ… Data Transformation"
        echo "  âœ… PostgreSQL Load"
        
    - name: Upload Pipeline Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: real-estate-data-${{ github.run_number }}
        path: |
          data/raw/*.csv
          data/transformed/*.csv
          etl_log/log.txt
        retention-days: 30
        
    - name: Send Success Notification
      if: success()
      run: |
        echo "âœ… Pipeline completed successfully"
        # Email is sent by your Python code
        
    - name: Send Failure Notification
      if: failure()
      run: |
        echo "âŒ Pipeline failed - check logs"

  cleanup:
    name: Cleanup & Monitoring
    runs-on: ubuntu-latest
    needs: [run-etl-pipeline]
    if: always()
    
    steps:
    - name: Pipeline Status Summary
      run: |
        echo "ğŸ“Š CI/CD Pipeline Summary"
        echo "========================="
        echo "Workflow: ${{ github.workflow }}"
        echo "Run Number: ${{ github.run_number }}"
        echo "Triggered by: ${{ github.event_name }}"
        echo "Branch: ${{ github.ref }}"
        echo "Commit: ${{ github.sha }}"