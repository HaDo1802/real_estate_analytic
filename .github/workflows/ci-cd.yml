name: Real Estate ETL Pipeline CI/CD

on:
  schedule:
    - cron: "0 6 * * *" # Daily at 6 AM UTC
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      run_full_pipeline:
        description: "Run full ETL pipeline (not just tests)"
        required: false
        default: "false"
        type: choice
        options:
          - "true"
          - "false"

env:
  PYTHON_VERSION: "3.9"

jobs:
  format-and-lint:
    name: Code Format & Lint
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install formatting tools
        run: |
          python -m pip install --upgrade pip
          pip install black isort

      - name: Format code with Black
        run: |
          echo "üé® Formatting code with Black..."
          black etl/ tests/ --line-length 127

      - name: Sort imports with isort
        run: |
          echo "üì¶ Sorting imports..."
          isort etl/ tests/ --profile black

      # No flake8, no linting failures, no noise
      # No auto-commit (pre-commit handles formatting locally)

  unit-test:
    name: Run Unit Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install pandas pytest

      - name: Run unit tests
        run: |
          pytest tests/test_*.py -v

      - name: Tests passed!
        run: echo "‚úÖ All tests passed!"

  docker-build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [format-and-lint, unit-test]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: ${{ github.event_name != 'pull_request' && secrets.DOCKER_USERNAME != '' }}
          tags: ${{ secrets.DOCKER_USERNAME }}/real-estate-etl:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

  run-etl-pipeline:
    runs-on: ubuntu-latest
    name: Execute Real Estate ETL Pipeline
    needs: [format-and-lint, unit-test, docker-build]
    if: github.ref == 'refs/heads/main' && (github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.run_full_pipeline == 'true'))

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests psycopg2-binary python-dotenv

      - name: Create data directories
        run: |
          mkdir -p data/raw data/transformed etl_log
          chmod 777 data/raw data/transformed etl_log

      - name: Configure environment (Connect to Local PostgreSQL)
        run: |
          # API Key (required for extract.py)
          echo "RAPID_API_KEY=${{ secrets.RAPID_API_KEY }}" > .env

          # PostgreSQL credentials from GitHub Secrets
          echo "POSTGRES_HOST=${{ secrets.POSTGRES_HOST }}" >> .env
          echo "POSTGRES_DB=${{ secrets.POSTGRES_DB }}" >> .env
          echo "POSTGRES_USER=${{ secrets.POSTGRES_USER }}" >> .env
          echo "POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD }}" >> .env
          echo "POSTGRES_PORT=${{ secrets.POSTGRES_PORT }}" >> .env

          # Schema name
          echo "DEFAULT_SCHEMA=real_estate_data" >> .env

          echo "‚úÖ Environment configured to connect to: ${{ secrets.POSTGRES_HOST }}:${{ secrets.POSTGRES_PORT }}/${{ secrets.POSTGRES_DB }}"

      - name: Step 1 - Extract Real Estate Data
        run: |
          echo "üï∑Ô∏è Starting Zillow data extraction..."
          cd etl
          python extract.py
          echo "‚úÖ Data extraction completed"

          if [ -f "../data/raw/raw_latest.csv" ]; then
            echo "üìÑ Raw data file created successfully"
            echo "Raw data rows: $(wc -l < ../data/raw/raw_latest.csv)"
          else
            echo "‚ùå Raw data file not found"
            exit 1
          fi

      - name: Step 2 - Transform and Clean Data
        run: |
          echo "üßπ Starting data transformation..."
          cd etl
          python transform.py
          echo "‚úÖ Data transformation completed"

          if [ -f "../data/transformed/transformed_latest.csv" ]; then
            echo "üìÑ Transformed data file created successfully"
            echo "Transformed data rows: $(wc -l < ../data/transformed/transformed_latest.csv)"
          else
            echo "‚ùå Transformed data file not found"
            exit 1
          fi

      - name: Step 3 - Load to PostgreSQL
        run: |
          echo "üóÑÔ∏è Starting PostgreSQL load..."
          cd etl
          python load.py
          echo "‚úÖ PostgreSQL load completed"

      - name: Verify Data Load
        run: |
          # Verify data was loaded to PostgreSQL
          PGPASSWORD=test_pass_123 psql -h localhost -U test_user -d test_real_estate -c "\dt real_estate_data.*" || echo "‚ö†Ô∏è Could not list tables"
          PGPASSWORD=test_pass_123 psql -h localhost -U test_user -d test_real_estate -c "SELECT COUNT(*) as total_records FROM real_estate_data.properties_data_history;" || echo "‚ö†Ô∏è Could not count records"

      - name: Pipeline Summary
        run: |
          echo "üéâ Real Estate ETL Pipeline Completed Successfully!"
          echo "üìÖ Executed on: $(date)"
          echo "üìä Pipeline Summary:"
          echo "  ‚úÖ Data Extraction (Zillow API)"
          echo "  ‚úÖ Data Transformation"
          echo "  ‚úÖ PostgreSQL Load"

          echo ""
          echo "üì¶ Data Files:"
          ls -lh data/raw/raw_latest.csv 2>/dev/null || echo "  Raw data: Not found"
          ls -lh data/transformed/transformed_latest.csv 2>/dev/null || echo "  Transformed data: Not found"

      - name: Upload Data Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: real-estate-data-${{ github.run_number }}
          path: |
            data/raw/*.csv
            data/transformed/*.csv
            etl_log/*.txt
          retention-days: 30
